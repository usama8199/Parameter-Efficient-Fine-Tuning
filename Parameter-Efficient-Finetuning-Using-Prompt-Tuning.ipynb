{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48e50301-a79c-47f5-91da-a619919e6704",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Prompt Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ed28f",
   "metadata": {},
   "source": [
    "\n",
    "# Prompt Tuning Tutorial\n",
    "\n",
    "This Jupyter notebook is designed to introduce you to the concept of prompt tuning, a method used to adapt language models to specific tasks or datasets without the need for extensive retraining. We'll go through setting up our environment, preparing our data, loading a model, and applying prompt tuning techniques.\n",
    "\n",
    "Prompt tuning offers a way to leverage large pre-trained models like GPT or BERT for specific tasks by fine-tuning them with prompts – small pieces of text that guide the model in generating responses or predictions in a desired context.\n",
    "\n",
    "In this tutorial, we will use a pre-trained model from the `transformers` library and apply prompt tuning to it for a specific task. The aim is to show how prompt tuning can be effectively used to guide the model's predictions.\n",
    "\n",
    "Paper can be found [here](https://arxiv.org/pdf/2104.08691.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8610c268-d8d4-4108-95b3-8bd43596c115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install transformers --upgrade  # Upgrade the transformers library to the latest version  # This command installs or upgrades the specified Python library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c799572",
   "metadata": {},
   "source": [
    "\n",
    "## Environment Setup\n",
    "\n",
    "Before we start, it's crucial to set up our environment by installing the necessary Python libraries. These libraries include `transformers` for accessing pre-trained models and utilities, `protobuf` for data serialization, and others that may be required for specific tasks. Uncomment any installations if you're working in an environment where these libraries are not already available.\n",
    "\n",
    "The code cells below install the necessary libraries. These installations are crucial for the subsequent parts of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e1842c-dca2-4804-919d-0438306c27bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install torch==1.7.*  # Optional: Install a specific version of PyTorch if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34025759-2776-4efe-8391-04532b029d94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pip install flash-attn  # Optional: Install flash attention for faster transformer computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d369ea-dd54-499f-80af-cb34a931d2f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install protobuf==3.20.*  # Install a specific version of protobuf required for compatibility  # This command installs or upgrades the specified Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60fc469c-bec5-4176-8702-4a62668ea60b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pip install torchvision==0.14.0  # Optional: Install a specific version of torchvision if working with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22678fe3-74ff-4fd7-a764-cf5f336f8f2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pip install flash_attn --upgrade  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6802ac1-88b6-478a-9dc7-ad7493c5dd62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install -U accelerate    # This command installs or upgrades the specified Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f953c3b3-9260-4732-bd6f-09dbdf2f37cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusparse-dev-11-7_11.7.3.50-1_amd64.deb -O /tmp/libcusparse-dev-11-7_11.7.3.50-1_amd64.deb && \\  \n",
    "#   wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcublas-dev-11-7_11.10.1.25-1_amd64.deb -O /tmp/libcublas-dev-11-7_11.10.1.25-1_amd64.deb && \\  \n",
    "#   wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcusolver-dev-11-7_11.4.0.1-1_amd64.deb -O /tmp/libcusolver-dev-11-7_11.4.0.1-1_amd64.deb && \\  \n",
    "#   wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcurand-dev-11-7_10.2.10.91-1_amd64.deb -O /tmp/libcurand-dev-11-7_10.2.10.91-1_amd64.deb && \\  \n",
    "#   dpkg -i /tmp/libcusparse-dev-11-7_11.7.3.50-1_amd64.deb && \\  \n",
    "#   dpkg -i /tmp/libcublas-dev-11-7_11.10.1.25-1_amd64.deb && \\  \n",
    "#   dpkg -i /tmp/libcusolver-dev-11-7_11.4.0.1-1_amd64.deb && \\  \n",
    "#   dpkg -i /tmp/libcurand-dev-11-7_10.2.10.91-1_amd64.deb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841a30f0-b16c-435f-8d87-8ebff9e7c79e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install torch==1.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfdac0e3-5443-41cd-828f-3431e4f43eaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94d7c249-5c01-4dbb-b02f-937a8a762bb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install torchaudio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ccd3983-37f5-44cb-9d62-25d374b00d2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install ninja  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf32198-a3c4-4c58-88b5-1bd6b8998dda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install flash-attn --no-build-isolation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d703bf8-9761-4243-a0a2-910650b3622f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.library.restartPython()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f80655-af26-48fb-a933-49871b3bd673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!flash-attn --version  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e449b509-c455-4572-bd7d-efaaa3ed3df1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data Preparation\n",
    "\n",
    "In this section, we will prepare our dataset for the prompt tuning process. This involves loading the data, preprocessing it according to the requirements of our model, and setting up training and validation splits. Data preparation is a crucial step to ensure our model can learn effectively from our dataset.\n",
    "    \n",
    "- **Load Data**: Load your dataset from a file or a database.\n",
    "- **Preprocess Data**: Clean and format your data to make it suitable for the model. This might involve tokenization, removing unnecessary parts of the data, or formatting it in a specific way.\n",
    "- **Split Data**: Divide your dataset into training and validation sets to evaluate the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad73fa70-9a5d-424f-b89d-d7c214f19a7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 11 18:46:28 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA A10G                    Off | 00000000:00:1E.0 Off |                    0 |\r\n",
      "|  0%   23C    P8               9W / 300W |      4MiB / 23028MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|  No running processes found                                                           |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "!nvidia-smi  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2271dfd-ca1d-47d8-832f-ad07fa1b0e5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch  \n",
    "torch.cuda.empty_cache()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45132b2c-5a8e-4e72-b423-316679e552bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os  \n",
    "  \n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:16'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d9104b-6593-4f71-9696-a6a86a401019",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset  \n",
    "  \n",
    "# dataset = load_dataset(\"wikisql\")  \n",
    "  \n",
    "# # Take only 1k data  \n",
    "# #dataset = dataset.select(range(2000))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d446ef9-ae69-4404-8d25-d70eababd0fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from llama_attn_replace import replace_llama_attn  \n",
    "# replace_llama_attn()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33806fc-635d-4aae-97c4-de666048193b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:27: UserWarning: This dataset can not be stored in DBFS because either `cache_dir` or the environment variable `HF_DATASETS_CACHE` is set to a non-DBFS path. If this cluster restarts, all saved dataset information will be lost.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a0ccff875b4837adc69440fe68dde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d80ad7907c4bc5b5c35c72af1f840d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d95496fa556447da4f1128edf29259a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c96ec909f55422a9f9671aef903ccf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f55b05a1944154b769b7d5520e127c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/15878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020de3d3b6c34332bc121d7c0112dd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/8421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da16b94350c477890b7bb6181b0a3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/56355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: Dataset({ features: ['phase', 'question', 'table', 'sql'], num_rows: 2000 })\n",
      "test: Dataset({ features: ['phase', 'question', 'table', 'sql'], num_rows: 500 })\n",
      "validation: Dataset({ features: ['phase', 'question', 'table', 'sql'], num_rows: 300 })\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict  \n",
    "  \n",
    "# Load the original dataset  \n",
    "original_dataset = load_dataset(\"wikisql\")  \n",
    "  \n",
    "# Define the sizes for the train, test, and validation splits  \n",
    "train_size = 2000  \n",
    "test_size = 500  \n",
    "validation_size = 300  \n",
    "  \n",
    "# Create new datasets for train, test, and validation  \n",
    "train_dataset = original_dataset[\"train\"].shuffle(seed=42).select([i for i in range(train_size)])  \n",
    "test_dataset = original_dataset[\"test\"].shuffle(seed=42).select([i for i in range(test_size)])  \n",
    "validation_dataset = original_dataset[\"validation\"].shuffle(seed=42).select([i for i in range(validation_size)])  \n",
    "  \n",
    "# Create a new DatasetDict and assign the sampled datasets to their respective keys  \n",
    "new_dataset_dict = DatasetDict({  \n",
    "    \"train\": train_dataset,  \n",
    "    \"test\": test_dataset,  \n",
    "    \"validation\": validation_dataset,  \n",
    "})  \n",
    "  \n",
    "# Print the sizes of the new datasets in the same format  \n",
    "for split, split_dataset in new_dataset_dict.items():  \n",
    "    print(f\"{split}: Dataset({{ features: {list(split_dataset.features.keys())}, num_rows: {len(split_dataset)} }})\")  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938451f8-5940-4cdf-8e3d-9e752fc584e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset=new_dataset_dict  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c163892-0f72-4b0c-8248-7181c8950c1e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Model Loading\n",
    "\n",
    "Here, we will load a pre-trained model and tokenizer from the `transformers` library. The choice of model depends on the task at hand and the language of your data. For example, for English text, GPT-3 or BERT models are commonly used.\n",
    "\n",
    "- **Load Tokenizer**: Load the tokenizer corresponding to your chosen model. This tokenizer will convert text into a format that the model can understand.\n",
    "- **Load Model**: Load the pre-trained model. We will fine-tune this model on our specific task using prompt tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b886d3ff-842e-4d0d-8f06-cd8a9d448a05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  \n",
    "\"cuda\" if torch.cuda.is_available() else \"cpu\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eafe3ee9-c4c6-4957-8b06-7e2f7cb29c3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup  \n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType  \n",
    "import torch  \n",
    "from datasets import load_dataset  \n",
    "import os  \n",
    "from torch.utils.data import DataLoader  \n",
    "from tqdm import tqdm  \n",
    "  \n",
    "device = \"cuda\"  \n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-hf\"  \n",
    "tokenizer_name_or_path = \"meta-llama/Llama-2-7b-hf\"  \n",
    "peft_config = PromptTuningConfig(  \n",
    "    task_type=TaskType.CAUSAL_LM,  \n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,  \n",
    "    num_virtual_tokens=8,  \n",
    "    prompt_tuning_init_text=\"###Instruction Convert question in natural language to SQL\",  \n",
    "    tokenizer_name_or_path=model_name_or_path,  \n",
    ")  \n",
    "  \n",
    "  \n",
    "text_column = \"question\"  \n",
    "label_column = \"human_readable\"  \n",
    "max_length = 64  \n",
    "lr = 3e-2  \n",
    "num_epochs = 3  \n",
    "batch_size = 8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ffa3f63-fd06-4eda-a8ea-61c127888c70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ee7e3b36d94015bdef45d6ddc5e6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login  \n",
    "notebook_login()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8f1a2d-91ab-48ed-8312-ea09611f7bbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce64cbbc7084f11af1dcfbf4168d0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50252fed8c4640b78e5bcafaac6d1be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbb742b3e894b0781e7d0a5d963dd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce91987a96749789b1c1fb3f705a29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)  \n",
    "if tokenizer.pad_token_id is None:  \n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4401e88-4100-4a03-9eeb-750e851c0d16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0664d0ef-8903-40d8-b30f-42f1b8f779c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Converting wikisql to instruction dataset\n",
    "\n",
    "```Format: Question : <Question> \\n Table Columns : <List_of_columns> \\n SQL : <SQL_TO_BE_GENERATED>```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99c3ebb7-141f-4ed9-b846-09de2d59a824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_table_context(table):  \n",
    "    header_type = [f\"{header}:{typ}\" for header,typ in zip(table['header'],table['types'])]  \n",
    "    return \",\".join(header_type)  \n",
    "  \n",
    "  \n",
    "def preprocess_function(examples):  \n",
    "    batch_size = len(examples[text_column])  \n",
    "    inputs = [f\"{text_column} : {x} \\n ###Table Columns : {fetch_table_context(t)} \\n ###SQL : \" for x,t in zip(examples[text_column],examples['table'])]   \n",
    "    print(inputs[0])  \n",
    "    targets = [str(x[label_column])+'\\n' for x in examples[\"sql\"]]  \n",
    "    model_inputs = tokenizer(inputs)  \n",
    "    labels = tokenizer(targets)  \n",
    "    for i in range(batch_size):  \n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]  \n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]  \n",
    "        # print(i, sample_input_ids, label_input_ids)  \n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids  \n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids  \n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])  \n",
    "    # print(model_inputs)  \n",
    "    for i in range(batch_size):  \n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]  \n",
    "        label_input_ids = labels[\"input_ids\"][i]  \n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (  \n",
    "            max_length - len(sample_input_ids)  \n",
    "        ) + sample_input_ids  \n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[  \n",
    "            \"attention_mask\"  \n",
    "        ][i]  \n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids  \n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])  \n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])  \n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])  \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]  \n",
    "    return model_inputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "911c13d7-90a5-4e3a-9f91-537509c0d617",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5416ae4bb42d4d52bfb74e5b84736c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question : Which sum of week that had an attendance larger than 55,767 on September 28, 1986? \n",
      " ###Table Columns : Week:real,Date:text,Opponent:text,Result:text,Attendance:real \n",
      " ###SQL : \n",
      "question : What is the sum of losses for Geelong Amateur, with 0 byes? \n",
      " ###Table Columns : Bellarine FL:text,Wins:real,Byes:real,Losses:real,Draws:real,Against:real \n",
      " ###SQL : \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611790f4a21d4abcbf0df3dd33cb137d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question : Name the incelandic of the glossary for 218 \n",
      " ###Table Columns : Word number:text,The Basque of the glossary:text,Modern Basque:text,The Icelandic of the glossary:text,English translation:text \n",
      " ###SQL : \n"
     ]
    }
   ],
   "source": [
    "train_processed_datasets = dataset['train'].map(  \n",
    "    preprocess_function,  \n",
    "    batched=True,  \n",
    "    num_proc=1,  \n",
    "    load_from_cache_file=False,  \n",
    "    remove_columns=dataset[\"train\"].column_names,  \n",
    "    desc=\"Running tokenizer on dataset\",  \n",
    ")  \n",
    "  \n",
    "test_processed_datasets = dataset['test'].map(  \n",
    "    preprocess_function,  \n",
    "    batched=True,  \n",
    "    num_proc=1,  \n",
    "    load_from_cache_file=False,  \n",
    "    remove_columns=dataset[\"test\"].column_names,  \n",
    "    desc=\"Running tokenizer on dataset\",  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19321858-10de-455b-b58b-d42951385c12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phase': 2,\n",
       " 'question': 'Which sum of week that had an attendance larger than 55,767 on September 28, 1986?',\n",
       " 'table': {'header': ['Week', 'Date', 'Opponent', 'Result', 'Attendance'],\n",
       "  'page_title': '1986 Kansas City Chiefs season',\n",
       "  'page_id': '12536732',\n",
       "  'types': ['real', 'text', 'text', 'text', 'real'],\n",
       "  'id': '2-12536732-1',\n",
       "  'section_title': 'Schedule',\n",
       "  'caption': 'Schedule',\n",
       "  'rows': [['1',\n",
       "    'September 7, 1986',\n",
       "    'Cincinnati Bengals',\n",
       "    'W 24–14',\n",
       "    '43,430'],\n",
       "   ['2', 'September 14, 1986', 'at Seattle Seahawks', 'L 23–17', '61,068'],\n",
       "   ['3', 'September 21, 1986', 'Houston Oilers', 'W 27–13', '43,699'],\n",
       "   ['4', 'September 28, 1986', 'at Buffalo Bills', 'W 20–17', '67,555'],\n",
       "   ['5', 'October 5, 1986', 'Los Angeles Raiders', 'L 24–17', '74,430'],\n",
       "   ['6', 'October 12, 1986', 'at Cleveland Browns', 'L 20–7', '71,278'],\n",
       "   ['7', 'October 19, 1986', 'San Diego Chargers', 'W 42–41', '55,767'],\n",
       "   ['8', 'October 26, 1986', 'Tampa Bay Buccaneers', 'W 27–20', '36,230'],\n",
       "   ['9', 'November 2, 1986', 'at San Diego Chargers', 'W 24–23', '48,518'],\n",
       "   ['10', 'November 9, 1986', 'Seattle Seahawks', 'W 27–7', '53,268'],\n",
       "   ['11', 'November 16, 1986', 'at Denver Broncos', 'L 38–17', '75,745'],\n",
       "   ['12', 'November 23, 1986', 'at St. Louis Cardinals', 'L 23–14', '29,680'],\n",
       "   ['13', 'November 30, 1986', 'Buffalo Bills', 'L 17–14', '31,492'],\n",
       "   ['14', 'December 7, 1986', 'Denver Broncos', 'W 37–10', '47,019'],\n",
       "   ['15', 'December 14, 1986', 'at Los Angeles Raiders', 'W 20–17', '60,952'],\n",
       "   ['16', 'December 21, 1986', 'at Pittsburgh Steelers', 'W 24–19', '47,150']],\n",
       "  'name': ''},\n",
       " 'sql': {'human_readable': 'SELECT SUM Week FROM table WHERE Attendance > 55,767 AND Date = september 28, 1986',\n",
       "  'sel': 0,\n",
       "  'agg': 4,\n",
       "  'conds': {'column_index': [4, 1],\n",
       "   'operator_index': [1, 0],\n",
       "   'condition': ['55,767', 'september 28, 1986']}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d4e927-a566-4eb2-bf1d-ff514169f4b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883\n",
      "128000\n"
     ]
    }
   ],
   "source": [
    "words=[word for sentence in train_processed_datasets['labels'] for word in sentence]  \n",
    "  \n",
    "# Get the set of unique words  \n",
    "unique_words = set(words)  \n",
    "  \n",
    "# Print the number of unique words  \n",
    "print(len(unique_words))  \n",
    "print(len(words))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5398e3b-f5b7-4207-95c7-06899b22cf34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5558\n",
      "128000\n"
     ]
    }
   ],
   "source": [
    "words=[word for sentence in train_processed_datasets['input_ids'] for word in sentence]  \n",
    "  \n",
    "# Get the set of unique words  \n",
    "unique_words = set(words)  \n",
    "  \n",
    "# Print the number of unique words  \n",
    "print(len(unique_words))  \n",
    "print(len(words))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2942d24-71db-4d01-a6f1-c819471bcbb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "words=[word for sentence in test_processed_datasets['labels'] for word in sentence]  \n",
    "  \n",
    "# Get the set of unique words  \n",
    "unique_words = set(words)  \n",
    "  \n",
    "# Print the number of unique words  \n",
    "print(len(unique_words))  \n",
    "print(len(words))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df304647-b4dc-4b1a-a7bf-264018ad0ac4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2805\n"
     ]
    }
   ],
   "source": [
    "words=[word for sentence in test_processed_datasets['input_ids'] for word in sentence]  \n",
    "  \n",
    "# Get the set of unique words  \n",
    "unique_words = set(words)  \n",
    "  \n",
    "# Print the number of unique words  \n",
    "print(len(unique_words))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f55c8e9-30b9-4b9e-876d-6588cbac1fa9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(  \n",
    "    train_processed_datasets, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True  \n",
    ")  \n",
    "eval_dataloader = DataLoader(test_processed_datasets, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prompt Tuning Implementation\n",
    "\n",
    "Prompt tuning is a lightweight method to fine-tune large language models on a specific task with minimal data. It involves adjusting the prompts that we provide to the model to steer its predictions in the desired direction.\n",
    "\n",
    "- **Define Prompts**: Create prompts that are relevant to your task. These prompts should guide the model in generating the correct output for your task.\n",
    "- **Tune Model with Prompts**: Apply these prompts to your model and adjust the model's responses based on the prompts. This step may involve training the model on a dataset with these prompts.\n",
    "- **Evaluate**: Test the model's performance on a validation set to see how well it has adapted to the task with the help of the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa9940e-4103-4c16-a0bb-e465c7788759",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc7941861194bfdb3aade34f9f22ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a88a9ae56142fe8a748623601a81a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 32,768 || all params: 6,738,448,384 || trainable%: 0.0004862840543203603\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from transformers import (  \n",
    "    AutoModelForCausalLM,  \n",
    "    AutoTokenizer,  \n",
    "    BitsAndBytesConfig,  \n",
    "    TrainingArguments,  \n",
    ")  \n",
    "  \n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type=\"nf4\",  \n",
    "    bnb_4bit_compute_dtype=torch.float16,  \n",
    ")  \n",
    "  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,trust_remote_code=True, device_map=\"auto\",quantization_config=bnb_config, torch_dtype=torch.float16)  \n",
    "model = get_peft_model(model, peft_config)  \n",
    "print(model.print_trainable_parameters())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff422487-f3a4-4034-a423-68a5dc7e2204",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)  \n",
    "lr_scheduler = get_linear_schedule_with_warmup(  \n",
    "    optimizer=optimizer,  \n",
    "    num_warmup_steps=0,  \n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),  \n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7a23c1-5dc0-43fc-a515-a4ad1234f8ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5ccc59-62c8-410c-a059-851c7ba9ce7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1878395f-4f5a-460f-a724-bed60952a9b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)  \n",
    "#Training the model  \n",
    "for epoch in range(num_epochs):  \n",
    "    model.train()  \n",
    "    total_loss = 0  \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):  \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}  \n",
    "        outputs = model(**batch)  \n",
    "        loss = outputs.loss  \n",
    "        total_loss += loss.detach().float()  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        lr_scheduler.step()  \n",
    "        optimizer.zero_grad()  \n",
    "  \n",
    "    model.eval()  \n",
    "    eval_loss = 0  \n",
    "    eval_preds = []  \n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):  \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}  \n",
    "        with torch.no_grad():  \n",
    "            outputs = model(**batch)  \n",
    "        loss = outputs.loss  \n",
    "        eval_loss += loss.detach().float()  \n",
    "        eval_preds.extend(  \n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)  \n",
    "        )  \n",
    "  \n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)  \n",
    "    eval_ppl = torch.exp(eval_epoch_loss)  \n",
    "    train_epoch_loss = total_loss / len(train_dataloader)  \n",
    "    train_ppl = torch.exp(train_epoch_loss)  \n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ee0007-7d03-44d4-a951-7a8d1ee27ca8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.mkdirs(\"save_model_path\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70b059f5-c422-4916-acf4-5d5d3724e87e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"save_model_path\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inference and Conclusion\n",
    "\n",
    "In this tutorial, we've covered the basics of prompt tuning, from setting up our environment, preparing our data, loading a pre-trained model, to applying prompt tuning techniques. The aim was to demonstrate how prompt tuning can be utilized to adapt large language models to specific tasks with relatively little data.\n",
    "\n",
    "Prompt tuning represents a powerful technique in the NLP toolkit, allowing for flexible and efficient model adaptation. We encourage you to experiment with different prompts and tasks to explore the full potential of this approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9679e4-8a39-4fdf-9c72-6f249a338260",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "from huggingface_hub import notebook_login  \n",
    "notebook_login()  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "451dcf1f-d492-4ab5-8256-dcff3918dbeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup,BitsAndBytesConfig  \n",
    "from peft import PeftModel,get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, PeftConfig  \n",
    "import torch  \n",
    "  \n",
    "device = \"cuda\"  \n",
    "peft_model_id = \"save_model_path\"\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-hf\"  \n",
    "  \n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type=\"nf4\",  \n",
    "    bnb_4bit_compute_dtype=torch.float16,  \n",
    ")  \n",
    "  \n",
    "  \n",
    "config = PeftConfig.from_pretrained(peft_model_id)  \n",
    "org_model = AutoModelForCausalLM.from_pretrained(model_name_or_path,trust_remote_code=True, device_map=\"auto\",quantization_config=bnb_config)  \n",
    "model = PeftModel.from_pretrained(org_model, peft_model_id)  \n",
    "model.eval()  \n",
    "model.to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec49df1-9fcc-4fcc-95d6-eac667dee041",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)  \n",
    "if tokenizer.pad_token_id is None:  \n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc68ed8-4785-498a-be82-c487bdfb08ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-2407677650979138>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m40\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], fetch_table_context(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m40\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m<command-2407677650979138>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m40\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], fetch_table_context(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m40\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\n\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'dataset' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "  \n",
    "dataset['test'][40]['question'], fetch_table_context(dataset['test'][40]['table'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c7321e-e97c-4c22-b1e1-4e97f3ab56ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stop_words_ids = [tokenizer.encode(stop_word) for stop_word in [\"\\n\"]]  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04095426-f88a-4647-8fe9-7085dbfefe42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def infer(model,input_text):  \n",
    "    inputs = tokenizer(input_text,return_tensors=\"pt\")  \n",
    "    with torch.no_grad():  \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}  \n",
    "        outputs = model.generate(  \n",
    "            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=40)  \n",
    "        print(outputs)  \n",
    "      \n",
    "    return tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)  \n",
    "  \n",
    "def parse(text):  \n",
    "    return text.split(\"###SQL :\")[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb2109b-7014-40ba-b255-2eb68bf82859",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/peft/peft_model.py:996: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   835, 12470,   584,  1724,   338,   278,  6588,  8158,   310,\n",
      "          8041,   297,  5844, 29879, 29871,    13,   835,  3562, 12481, 29879,\n",
      "           584,  1178, 29901, 10192, 29892,   978, 29901,   726, 29892, 16009,\n",
      "         29901,  1626, 29892, 13628, 29901, 10192,    13,   835,  4176,   584,\n",
      "         29871,     1, 14262,  7228,    13, 26077,    13, 23196,  4345,  7228,\n",
      "          4345,    13, 25145,    13, 23196,    13, 14332,    13, 26077,  4345,\n",
      "          7228, 29905,    13, 26077,    13, 26077,  4345,    13, 26502,  4345,\n",
      "          7228,  4345,    13, 23196,    13, 23196,    13, 19379,  4345,    13,\n",
      "         27581]], device='cuda:0')\n",
      "  февPA\n",
      " everybody\n",
      " nobodyMSPAMS\n",
      " kwiet\n",
      " nobody\n",
      " everyone\n",
      " everybodyMSPA\\\n",
      " everybody\n",
      " everybodyMS\n",
      " вересняMSPAMS\n",
      " nobody\n",
      " nobody\n",
      " BegriffeMS\n",
      " hopefully\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"###question : What is the average score of students in maths   \n",
    " ###Table Columns : id:INT,name:text,subject:Text,score:INT  \n",
    " ###SQL : \"\"\"  \n",
    "   \n",
    "predictions = infer(model,input_text)  \n",
    "print(parse(predictions[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60daf727-da85-4c07-8e18-18997ca86740",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sierpP. kwietPAMS\n",
      " hopefullyMS\n",
      " ultimately\n",
      " surelyMS\n",
      " HinweisMS\n",
      " nobody. броја \n",
      " січняOPAMS \n",
      " nobodyMS\n",
      " sierp. sierp\n",
      " paździer\n",
      " nobody\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"###question : What is the highest score of dhoni in a match in chennai  \n",
    " ###Table Columns : id:INT,player:text,runs:INT,match:INT,year:INT,city:text  \n",
    " ###SQL : \"\"\"  \n",
    "   \n",
    "predictions = infer(model,input_text)  \n",
    "print(parse(predictions[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e3d281-7e38-4e3d-9211-e5796c205e8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bec524067c4def95a7428aaf7232f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (  \n",
    "    AutoModelForCausalLM,  \n",
    "    AutoTokenizer,  \n",
    "    BitsAndBytesConfig,  \n",
    "    TrainingArguments,  \n",
    ")  \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup  \n",
    "from peft import PeftModel,get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, PeftConfig  \n",
    "import torch  \n",
    "  \n",
    "device = \"cuda\"  \n",
    "peft_model_id = \"save_model_path\"\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-hf\"  \n",
    "  \n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_quant_type=\"nf4\",  \n",
    "    bnb_4bit_compute_dtype=torch.float16,  \n",
    ")  \n",
    "  \n",
    "model_untuned = AutoModelForCausalLM.from_pretrained(model_name_or_path,trust_remote_code=True, device_map=\"auto\",quantization_config=bnb_config,use_flash_attention_2=True)  \n",
    "# model = get_peft_model(model, peft_config)  \n",
    "# print(model.print_trainable_parameters())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48740dbd-c265-4630-bd37-8be270272c52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   835,  3379,  4080, 14806,  1139,   297,  5613,  4086,   304,\n",
      "          3758,    13,  2277, 29937, 12470,   584,  1724,   338,   278,  6588,\n",
      "          8158,   310,  8041,   297,  5844, 29879, 29871,    13,   835,  3562,\n",
      "         12481, 29879,   584,  1178, 29901, 10192, 29892,   978, 29901,   726,\n",
      "         29892, 16009, 29901,  1626, 29892, 13628, 29901, 10192,    13,   835,\n",
      "          4176,   584, 29871,    13, 29871, 29914, 29991, 29871,   306, 29892,\n",
      "            13,  8778,   313,    13,   435, 29892,   313, 29896, 29933, 29874,\n",
      "         29871, 29871, 29892,    13,  2648, 29906, 29889,  8778,    13, 29871,\n",
      "         29892,   491,   491, 29871,   313,   313, 29892,   304,   491, 29889,\n",
      "           922,    13,  3148]], device='cuda:0')\n",
      " \n",
      " /!  I,\n",
      " Home (\n",
      " J, (1Ba  ,\n",
      " By2. Home\n",
      " , by by  ( (, to by. Se\n",
      " US\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"###Instruction Convert question in natural language to SQL  \n",
    "###question : What is the average score of students in maths   \n",
    " ###Table Columns : id:INT,name:text,subject:Text,score:INT  \n",
    " ###SQL : \"\"\"  \n",
    "   \n",
    "predictions = infer(model_untuned,input_text)  \n",
    "print(parse(predictions[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b879719-378c-4b2f-a872-8c1bffe10acc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"###Instruction Convert question in natural language to SQL  \n",
    "###question : What is the highest score of dhoni in a match in chennai  \n",
    " ###Table Columns : id:INT,player:text,runs:INT,match:INT,year:INT,city:text  \n",
    " ###SQL : \"\"\"  \n",
    "   \n",
    "predictions = infer(model_untuned,input_text)  \n",
    "print(parse(predictions[0]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66af804c-3d8b-46f4-83b2-7d3904b5e1ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Input Question and Table Schema**\\\n",
    " What is the average score of students in maths \\\n",
    " (id:INT, name:text, subject:Text, score:INT)\n",
    "\n",
    "**Actual Output**\\\n",
    "SELECT AVG(score) FROM table WHERE subject = 'maths'\n",
    "\n",
    "**Hard Prompt Output**\\\n",
    "select avg(score) from \n",
    "  (select id,name,subject,score from \n",
    "  (select id,name,subject,score from \n",
    "  (select\n",
    "\n",
    " **Prompt Tuned Output**\\\n",
    " 'SELECT AVG score FROM table WHERE subject = maths'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dbb3b85-92d3-4ffe-9437-af156073da0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Input Question and Table Schema**\\\n",
    "\n",
    " What is the highest score of dhoni in a match in chennai \\\n",
    " (id:INT,player:text,runs:INT,match:INT,year:INT,city:text)\n",
    "\n",
    "**Actual Output**\\\n",
    "SELECT MAX(runs) FROM table WHERE match = 'chennai' AND player = 'dhoni'\n",
    "\n",
    "**Hard Prompt Output**\\\n",
    "SELECT player,runs,year,city FROM cricket_matches WHERE player='Dhoni' AND year='2010' AND city='Chennai' AND runs='\n",
    "\n",
    " **Prompt Tuned Output**\\\n",
    " 'SELECT MAX runs  FROM table WHERE match = chennai AND player = dhoni '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad72aa3-06f8-46bb-aab8-4fe813cb9895",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Parameter:** \\\n",
    " trainable params: 32,768 || all params: 6,738,448,384 || trainable%: 0.0004862840543203603\n",
    " \n",
    "**Computation Summary** :\\\n",
    "Worker Type: g4dn.xlarge(16GB Memory 1GPU) 2-8 Workers\\\n",
    "Driver Type: g4dn.xlarge(16GB Memory 1GPU)\n",
    "\n",
    "**Time Took to train**\n",
    "15 Minutes\n",
    "\n",
    "**Unique Tokens**\n",
    "15k Tokens\n",
    "\n",
    "**Non-Unique Tokens**\n",
    "150k Tokens"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Prompt_tuning",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
